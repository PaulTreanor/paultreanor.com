{"pageProps":{"postData":{"id":"llamafile","contentHtml":"<h1>Automatically running LLMs at startup on Mac</h1>\n<p><strong>March 2024 Update</strong>: <a href=\"https://lmstudio.ai/\">Thankfully there's better ways to run this sort of things now!</a></p>\n<p><a href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\">Llamafile</a> lets you run a local LLM as an executable file that launches a chat UI on a localhost port. This gave me the idea to keep the LLM always running in the background, so I can bookmark the port in my browser and ask it questions with the click of a button.</p>\n<p>Overall the model I'm running (Llava, a fine tune of LLama) is pretty impressive and it's very fast on my M1 MacBook. The model is great for generating names (better that GPT-4 in my experience) and very good at explaining technical topics, but its programming skills need some work.</p>\n<p>Here are the steps make a really convenient local LLM on MacOS:</p>\n<ol>\n<li><strong>Download an LLM executable with Llamafile</strong></li>\n</ol>\n<p><a href=\"https://simonwillison.net/2023/Nov/29/llamafile/\">Simon Willison's blog post</a> has great instructions for this (his blog is always fantastic btw), but here's a quick summary:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Download the file</span>\ncurl -LO https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-server.llamafile\n\n<span class=\"hljs-comment\"># Make it executable</span>\n<span class=\"hljs-built_in\">chmod</span> 755 llava-v1.5-7b-q4-server.llamafile\n\n<span class=\"hljs-comment\"># Run the file</span>\n./llava-v1.5-7b-q4-server.llamafile\n\n<span class=\"hljs-comment\"># Go to http://127.0.0.1:8080/ to play around with the model. </span>\n</code></pre>\n<ol start=\"2\">\n<li>\n<p><strong>Install Xbar</strong>\nWe're going to use a utility called <a href=\"https://xbarapp.com/\">Xbar</a> to run Llamafile in the background at startup. Just download the Xbar <code class=\"inline-code-custom-style\">.dmg</code> and put it in your Applications folder.</p>\n</li>\n<li>\n<p><strong>Create an Xbar plugin</strong></p>\n</li>\n</ol>\n<p>An Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches it will run these scripts.</p>\n<p>Go to the xbar plugins directory:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Go to the xbar plugins directory</span>\n<span class=\"hljs-built_in\">cd</span> ~/Library/Application\\ Support/xbar/plugins\n\n<span class=\"hljs-comment\"># Create a bash file called runLlava.1d.sh</span>\n<span class=\"hljs-built_in\">touch</span> runLlava.1d.sh\n\n<span class=\"hljs-comment\"># Make the file executable</span>\n<span class=\"hljs-built_in\">chmod</span> +x runLlava.1d.sh\n</code></pre>\n<p>Now we can add some code to <code class=\"inline-code-custom-style\">runLlava.1d.sh</code> that calls the Llamafile executable you downloaded in step 1:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-meta\">#!/bin/bash</span>\n<span class=\"hljs-built_in\">nohup</span> /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 &#x26;\n</code></pre>\n<p>At this point you should test the <code class=\"inline-code-custom-style\">runLlava.1d.sh</code> out.</p>\n<pre><code class=\"hljs language-bash\">\n./runLlava.1d.sh\n\n<span class=\"hljs-comment\"># Go to http://127.0.0.1:8080/ to make sure it's working</span>\n</code></pre>\n<ol start=\"4\">\n<li><strong>Bringing it all together</strong>\n<a href=\"https://www.idownloadblog.com/2015/03/24/apps-launch-system-startup-mac/\">Set xbar to run on startup</a> and then restart your machine.</li>\n</ol>\n<p>Then bookmark localhost:8080 in your browser so you can access it quickly. The model will always be available to answer your questions, just like to ChatGPT...but without the downtime.</p>\n<p><br>\n</p>\n<img src=\"/images/llamafile/bookmark.png\" alt=\"llamafile web UI and bookmark\">\n<h2>Update</h2>\n<p>This absolutely chews through my battery life even though it's idle in the background. I think making the script a little more event driven (maybe triggered by visiting port 8080) and not just running idle should be a decent fix for this.</p>","title":"Automatically running LLMs at startup on Mac","short":"Setting up a local LLM on Mac so that it's always ready for your questions","date":"2023-12-01","slug":"llamafile","createdAt":"2023-12-01","img":"blog-2.jpg","tags":["Tutorial"]}},"__N_SSG":true}