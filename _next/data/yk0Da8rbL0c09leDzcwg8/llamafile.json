{"pageProps":{"postData":{"id":"llamafile","contentHtml":"<h1>Automatically running LLMs on startup on Mac</h1>\n<p><a href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\">Llamafile</a> lets you run a local LLM as a single downloadable executable and chat with it on a localhost port. This gave me the idea to make this even more accessible by making the LLM always be running so I can bookmark the port and chat to it instantly at any time, like a backup ChatGPT.</p>\n<p>It's fun experiment, and the LLM I'm running (Llava) is pretty impressive. On my M1 MacBook it is faster than ChatGPT (although not quite as smart with most tasks). Best of all it isn't that hard to setup.</p>\n<ol>\n<li><strong>Download an LLM executable with Llamafile</strong></li>\n</ol>\n<p>This is very simple and <a href=\"https://simonwillison.net/2023/Nov/29/llamafile/\">Simon Willison's blog post</a> has great instructions for this. You want to download the file, make it executable with <code>chmod</code>, and then run the file to make sure it works.</p>\n<ol start=\"2\">\n<li><strong>Install Xbar</strong></li>\n</ol>\n<p>Next up install <a href=\"https://xbarapp.com/\">Xbar</a>. Xbar is a utility that lets you run launch agent scripts very easily on MacOS. Just download the <code>.dmg</code> and put it in your applications folder.</p>\n<ol start=\"3\">\n<li><strong>Create an Xbar plugin</strong></li>\n</ol>\n<p>An Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches (it should launch on startup) it will run these scripts.</p>\n<p>Go to the xbar plugins directory:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">cd</span> ~/Library/Application\\ Support/xbar/plugins\n</code></pre>\n<p>Create a your file in this directory called <code>runLlava.1d.sh</code> and make the file executable.</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">touch</span> runLlava.1d.sh\n<span class=\"hljs-built_in\">chmod</span> +x runLlava.1d.sh\n</code></pre>\n<p>Then add some bash code that just calls the llamafile executable you downloaded in step 1:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-meta\">#!/bin/bash</span>\n<span class=\"hljs-built_in\">nohup</span> /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 &#x26;\n</code></pre>\n<p>At this point you should test the script out.</p>\n<pre><code class=\"hljs language-bash\">./runLlava.1d.sh\n\n<span class=\"hljs-comment\"># Then go to localhost:8080 to see the UI</span>\n</code></pre>\n<ol start=\"4\">\n<li><strong>Test it out</strong></li>\n</ol>\n<p>Restart your machine and launch Xbar. I have Xbar set so it runs when I turn my Mac on.</p>\n<p>Then bookmark localhost:8080 in your browser so you can access it whenever you want, just like ChatGPT.</p>\n<img src=\"/images/llamafile/bookmark.png\" alt=\"llamafile web UI and bookmark\">","title":"Automatically running LLMs on startup on Mac","short":"Setting up a local LLM on Mac so that it's always ready for your questions","date":"2023-12-01","slug":"llamafile","createdAt":"2023-12-01","img":"blog-2.jpg","tags":["Tutorial"]}},"__N_SSG":true}