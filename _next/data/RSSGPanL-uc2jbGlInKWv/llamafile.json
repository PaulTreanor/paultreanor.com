{"pageProps":{"postData":{"id":"llamafile","contentHtml":"<p><a href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\">Llamafile</a> lets you run a local LLM as an executable file that launches a chat UI on a localhost port. This gave me the idea to keep the LLM always running in the background, so I can bookmark the port in my browser and ask it questions with the click of a button.</p>\n<p>Overall the model I'm running (Llava) is pretty impressive and it's very fast on my M1 MacBook. The model is great for generating names (better that GPT-4 imo) and very good at explaining technical topcs, but it's programming skills need some work.</p>\n<p>Here's the steps make using a local LLM really convenient on MacOS.</p>\n<ol>\n<li><strong>Download an LLM executable with Llamafile</strong></li>\n</ol>\n<p><a href=\"https://simonwillison.net/2023/Nov/29/llamafile/\">Simon Willison's blog post</a> has great instructions for this (his blog is always fantastic btw), but here's a quick summary:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Download the file</span>\ncurl -LO https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-server.llamafile\n\n<span class=\"hljs-comment\"># Make it executable</span>\n<span class=\"hljs-built_in\">chmod</span> 755 llava-v1.5-7b-q4-server.llamafile\n\n<span class=\"hljs-comment\"># Run the file</span>\n./llava-v1.5-7b-q4-server.llamafile\n\n<span class=\"hljs-comment\"># Go to http://127.0.0.1:8080/ to play around with the model. </span>\n</code></pre>\n<ol start=\"2\">\n<li>\n<p><strong>Install Xbar</strong>\nWe're going to use a utility called <a href=\"https://xbarapp.com/\">Xbar</a> to run Llamafile in the background at startup. Just download the Xbar <code>.dmg</code> and put it in your applications folder.</p>\n</li>\n<li>\n<p><strong>Create an Xbar plugin</strong></p>\n</li>\n</ol>\n<p>An Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches it will run these scripts.</p>\n<p>Go to the xbar plugins directory:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Go to the xbar plugins directory</span>\n<span class=\"hljs-built_in\">cd</span> ~/Library/Application\\ Support/xbar/plugins\n\n<span class=\"hljs-comment\"># Create a bash file called runLlava.1d.sh</span>\n<span class=\"hljs-built_in\">touch</span> runLlava.1d.sh\n\n<span class=\"hljs-comment\"># Make the file executable</span>\n<span class=\"hljs-built_in\">chmod</span> +x runLlava.1d.sh\n</code></pre>\n<p>Now we can add some code to runLlava.1d.sh that calls the llamafile executable you downloaded in step 1:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-meta\">#!/bin/bash</span>\n<span class=\"hljs-built_in\">nohup</span> /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 &#x26;\n</code></pre>\n<p>At this point you should test the runllava.1d.sh out.</p>\n<pre><code class=\"hljs language-bash\">\n./runLlava.1d.sh\n\n<span class=\"hljs-comment\"># Go to http://127.0.0.1:8080/ to make sure it's working</span>\n</code></pre>\n<ol start=\"4\">\n<li><strong>Brining it all together</strong>\n<a href=\"https://www.idownloadblog.com/2015/03/24/apps-launch-system-startup-mac/\">Set xbar to run on startup</a> and then restart you machine.</li>\n</ol>\n<p>Then bookmark localhost:8080 in your browser so you can access it quickly. The model will always be waiting to answer you questions, just like ChatGPT (without the downtime).</p>\n<p><br>\n</p>\n<img src=\"/images/llamafile/bookmark.png\" alt=\"llamafile web UI and bookmark\">","title":"Automatically running LLMs on startup on Mac","short":"Setting up a local LLM on Mac so that it's always ready for your questions","date":"2023-12-01","slug":"llamafile","createdAt":"2023-12-01","img":"blog-2.jpg","tags":["Tutorial"]}},"__N_SSG":true}