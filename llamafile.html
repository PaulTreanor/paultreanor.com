<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="My personal website where I keep my notes and thoughts."/><meta name="og:title" content="Paul Treanor"/><meta name="twitter:card" content="summary_large_image"/><link rel="alternate" type="application/rss+xml" title="RSS Feed for paultreanor.com" href="/rss.xml"/><title>Automatically running LLMs on startup on Mac</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/b09cab8ac293d967.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b09cab8ac293d967.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-481ef0d25a716735.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1693ab7fc31cdd21.js" defer=""></script><script src="/_next/static/chunks/562-d0bcd15ec5190dc9.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bid%5D-bf7a332e2e64ec22.js" defer=""></script><script src="/_next/static/yk0Da8rbL0c09leDzcwg8/_buildManifest.js" defer=""></script><script src="/_next/static/yk0Da8rbL0c09leDzcwg8/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script><script nomodule="" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script><div class="mx-2 md:mx-20 lg:mx-40 font-open-sans mb-32"><header class="max-w-2xl mx-auto"><div class="mt-10 mb-8"><a href="/"><div class="text-4xl">üè†</div></a></div></header><main><div class="max-w-2xl mx-auto"><h5 class="text-slate-600 font-normal mb-5"><time dateTime="2023-12-01">December 1, 2023</time></h5><div class="blog-content"><h1>Automatically running LLMs on startup on Mac</h1>
<p><a href="https://hacks.mozilla.org/2023/11/introducing-llamafile/">Llamafile</a> lets you run a local LLM as a single downloadable executable and chat with it on a localhost port. This gave me the idea to make this even more accessible by making the LLM always be running so I can bookmark the port and chat to it instantly at any time, like a backup ChatGPT.</p>
<p>It's fun experiment, and the LLM I'm running (Llava) is pretty impressive. On my M1 MacBook it is faster than ChatGPT (although not quite as smart with most tasks). Best of all it isn't that hard to setup.</p>
<ol>
<li><strong>Download an LLM executable with Llamafile</strong></li>
</ol>
<p>This is very simple and <a href="https://simonwillison.net/2023/Nov/29/llamafile/">Simon Willison's blog post</a> has great instructions for this. You want to download the file, make it executable with <code>chmod</code>, and then run the file to make sure it works.</p>
<ol start="2">
<li><strong>Install Xbar</strong></li>
</ol>
<p>Next up install <a href="https://xbarapp.com/">Xbar</a>. Xbar is a utility that lets you run launch agent scripts very easily on MacOS. Just download the <code>.dmg</code> and put it in your applications folder.</p>
<ol start="3">
<li><strong>Create an Xbar plugin</strong></li>
</ol>
<p>An Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches (it should launch on startup) it will run these scripts.</p>
<p>Go to the xbar plugins directory:</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">cd</span> ~/Library/Application\ Support/xbar/plugins
</code></pre>
<p>Create a your file in this directory called <code>runLlava.1d.sh</code> and make the file executable.</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">touch</span> runLlava.1d.sh
<span class="hljs-built_in">chmod</span> +x runLlava.1d.sh
</code></pre>
<p>Then add some bash code that just calls the llamafile executable you downloaded in step 1:</p>
<pre><code class="hljs language-bash"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-built_in">nohup</span> /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 &#x26;
</code></pre>
<p>At this point you should test the script out.</p>
<pre><code class="hljs language-bash">./runLlava.1d.sh

<span class="hljs-comment"># Then go to localhost:8080 to see the UI</span>
</code></pre>
<ol start="4">
<li><strong>Test it out</strong></li>
</ol>
<p>Restart your machine and launch Xbar. I have Xbar set so it runs when I turn my Mac on.</p>
<p>Then bookmark localhost:8080 in your browser so you can access it whenever you want, just like ChatGPT.</p>
<img src="/images/llamafile/bookmark.png" alt="llamafile web UI and bookmark"></div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"llamafile","contentHtml":"\u003ch1\u003eAutomatically running LLMs on startup on Mac\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\"\u003eLlamafile\u003c/a\u003e lets you run a local LLM as a single downloadable executable and chat with it on a localhost port. This gave me the idea to make this even more accessible by making the LLM always be running so I can bookmark the port and chat to it instantly at any time, like a backup ChatGPT.\u003c/p\u003e\n\u003cp\u003eIt's fun experiment, and the LLM I'm running (Llava) is pretty impressive. On my M1 MacBook it is faster than ChatGPT (although not quite as smart with most tasks). Best of all it isn't that hard to setup.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDownload an LLM executable with Llamafile\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis is very simple and \u003ca href=\"https://simonwillison.net/2023/Nov/29/llamafile/\"\u003eSimon Willison's blog post\u003c/a\u003e has great instructions for this. You want to download the file, make it executable with \u003ccode\u003echmod\u003c/code\u003e, and then run the file to make sure it works.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eInstall Xbar\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNext up install \u003ca href=\"https://xbarapp.com/\"\u003eXbar\u003c/a\u003e. Xbar is a utility that lets you run launch agent scripts very easily on MacOS. Just download the \u003ccode\u003e.dmg\u003c/code\u003e and put it in your applications folder.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003eCreate an Xbar plugin\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches (it should launch on startup) it will run these scripts.\u003c/p\u003e\n\u003cp\u003eGo to the xbar plugins directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e ~/Library/Application\\ Support/xbar/plugins\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a your file in this directory called \u003ccode\u003erunLlava.1d.sh\u003c/code\u003e and make the file executable.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003etouch\u003c/span\u003e runLlava.1d.sh\n\u003cspan class=\"hljs-built_in\"\u003echmod\u003c/span\u003e +x runLlava.1d.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen add some bash code that just calls the llamafile executable you downloaded in step 1:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-meta\"\u003e#!/bin/bash\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003enohup\u003c/span\u003e /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 \u0026#x26;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point you should test the script out.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e./runLlava.1d.sh\n\n\u003cspan class=\"hljs-comment\"\u003e# Then go to localhost:8080 to see the UI\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eTest it out\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eRestart your machine and launch Xbar. I have Xbar set so it runs when I turn my Mac on.\u003c/p\u003e\n\u003cp\u003eThen bookmark localhost:8080 in your browser so you can access it whenever you want, just like ChatGPT.\u003c/p\u003e\n\u003cimg src=\"/images/llamafile/bookmark.png\" alt=\"llamafile web UI and bookmark\"\u003e","title":"Automatically running LLMs on startup on Mac","short":"Setting up a local LLM on Mac so that it's always ready for your questions","date":"2023-12-01","slug":"llamafile","createdAt":"2023-12-01","img":"blog-2.jpg","tags":["Tutorial"]}},"__N_SSG":true},"page":"/[id]","query":{"id":"llamafile"},"buildId":"yk0Da8rbL0c09leDzcwg8","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>