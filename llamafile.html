<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><meta name="description" content="My personal website where I keep my notes and thoughts."/><meta name="og:title" content="Paul Treanor"/><meta name="twitter:card" content="summary_large_image"/><link rel="alternate" type="application/rss+xml" title="RSS Feed for paultreanor.com" href="/rss.xml"/><title>Automatically running LLMs at startup on Mac</title><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/a0a5c827be6f67e6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a0a5c827be6f67e6.css" data-n-g=""/><link rel="preload" href="/_next/static/css/d0f3a9e7dfd4b19b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d0f3a9e7dfd4b19b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-481ef0d25a716735.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1693ab7fc31cdd21.js" defer=""></script><script src="/_next/static/chunks/562-d0bcd15ec5190dc9.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bid%5D-331c202ad6dd6bd9.js" defer=""></script><script src="/_next/static/HU6vXSPKOkwvEroM8TGj-/_buildManifest.js" defer=""></script><script src="/_next/static/HU6vXSPKOkwvEroM8TGj-/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script><script nomodule="" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script><div class="mx-2 md:mx-20 lg:mx-40 font-open-sans mb-32"><header class="max-w-2xl mx-auto"><div class="mt-10 mb-8"><a href="/"><div class="text-4xl">üè†</div></a></div></header><main><div class="max-w-2xl mx-auto"><h5 class="text-slate-600 font-normal mb-5"><time dateTime="2023-12-01">December 1, 2023</time></h5><div class="blog-content"><h1>Automatically running LLMs at startup on Mac</h1>
<p><strong>March 2024 Update</strong>: <a href="https://lmstudio.ai/">Thankfully there's better ways to run this sort of things now!</a></p>
<p><a href="https://hacks.mozilla.org/2023/11/introducing-llamafile/">Llamafile</a> lets you run a local LLM as an executable file that launches a chat UI on a localhost port. This gave me the idea to keep the LLM always running in the background, so I can bookmark the port in my browser and ask it questions with the click of a button.</p>
<p>Overall the model I'm running (Llava, a fine tune of LLama) is pretty impressive and it's very fast on my M1 MacBook. The model is great for generating names (better that GPT-4 in my experience) and very good at explaining technical topics, but its programming skills need some work.</p>
<p>Here are the steps make a really convenient local LLM on MacOS:</p>
<ol>
<li><strong>Download an LLM executable with Llamafile</strong></li>
</ol>
<p><a href="https://simonwillison.net/2023/Nov/29/llamafile/">Simon Willison's blog post</a> has great instructions for this (his blog is always fantastic btw), but here's a quick summary:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Download the file</span>
curl -LO https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-server.llamafile

<span class="hljs-comment"># Make it executable</span>
<span class="hljs-built_in">chmod</span> 755 llava-v1.5-7b-q4-server.llamafile

<span class="hljs-comment"># Run the file</span>
./llava-v1.5-7b-q4-server.llamafile

<span class="hljs-comment"># Go to http://127.0.0.1:8080/ to play around with the model. </span>
</code></pre>
<ol start="2">
<li>
<p><strong>Install Xbar</strong>
We're going to use a utility called <a href="https://xbarapp.com/">Xbar</a> to run Llamafile in the background at startup. Just download the Xbar <code class="inline-code-custom-style">.dmg</code> and put it in your Applications folder.</p>
</li>
<li>
<p><strong>Create an Xbar plugin</strong></p>
</li>
</ol>
<p>An Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches it will run these scripts.</p>
<p>Go to the xbar plugins directory:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Go to the xbar plugins directory</span>
<span class="hljs-built_in">cd</span> ~/Library/Application\ Support/xbar/plugins

<span class="hljs-comment"># Create a bash file called runLlava.1d.sh</span>
<span class="hljs-built_in">touch</span> runLlava.1d.sh

<span class="hljs-comment"># Make the file executable</span>
<span class="hljs-built_in">chmod</span> +x runLlava.1d.sh
</code></pre>
<p>Now we can add some code to <code class="inline-code-custom-style">runLlava.1d.sh</code> that calls the Llamafile executable you downloaded in step 1:</p>
<pre><code class="hljs language-bash"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-built_in">nohup</span> /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 &#x26;
</code></pre>
<p>At this point you should test the <code class="inline-code-custom-style">runLlava.1d.sh</code> out.</p>
<pre><code class="hljs language-bash">
./runLlava.1d.sh

<span class="hljs-comment"># Go to http://127.0.0.1:8080/ to make sure it's working</span>
</code></pre>
<ol start="4">
<li><strong>Bringing it all together</strong>
<a href="https://www.idownloadblog.com/2015/03/24/apps-launch-system-startup-mac/">Set xbar to run on startup</a> and then restart your machine.</li>
</ol>
<p>Then bookmark localhost:8080 in your browser so you can access it quickly. The model will always be available to answer your questions, just like to ChatGPT...but without the downtime.</p>
<p><br>
</p>
<img src="/images/llamafile/bookmark.png" alt="llamafile web UI and bookmark">
<h2>Update</h2>
<p>This absolutely chews through my battery life even though it's idle in the background. I think making the script a little more event driven (maybe triggered by visiting port 8080) and not just running idle should be a decent fix for this.</p></div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"llamafile","contentHtml":"\u003ch1\u003eAutomatically running LLMs at startup on Mac\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eMarch 2024 Update\u003c/strong\u003e: \u003ca href=\"https://lmstudio.ai/\"\u003eThankfully there's better ways to run this sort of things now!\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\"\u003eLlamafile\u003c/a\u003e lets you run a local LLM as an executable file that launches a chat UI on a localhost port. This gave me the idea to keep the LLM always running in the background, so I can bookmark the port in my browser and ask it questions with the click of a button.\u003c/p\u003e\n\u003cp\u003eOverall the model I'm running (Llava, a fine tune of LLama) is pretty impressive and it's very fast on my M1 MacBook. The model is great for generating names (better that GPT-4 in my experience) and very good at explaining technical topics, but its programming skills need some work.\u003c/p\u003e\n\u003cp\u003eHere are the steps make a really convenient local LLM on MacOS:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDownload an LLM executable with Llamafile\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://simonwillison.net/2023/Nov/29/llamafile/\"\u003eSimon Willison's blog post\u003c/a\u003e has great instructions for this (his blog is always fantastic btw), but here's a quick summary:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Download the file\u003c/span\u003e\ncurl -LO https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-server.llamafile\n\n\u003cspan class=\"hljs-comment\"\u003e# Make it executable\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003echmod\u003c/span\u003e 755 llava-v1.5-7b-q4-server.llamafile\n\n\u003cspan class=\"hljs-comment\"\u003e# Run the file\u003c/span\u003e\n./llava-v1.5-7b-q4-server.llamafile\n\n\u003cspan class=\"hljs-comment\"\u003e# Go to http://127.0.0.1:8080/ to play around with the model. \u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstall Xbar\u003c/strong\u003e\nWe're going to use a utility called \u003ca href=\"https://xbarapp.com/\"\u003eXbar\u003c/a\u003e to run Llamafile in the background at startup. Just download the Xbar \u003ccode class=\"inline-code-custom-style\"\u003e.dmg\u003c/code\u003e and put it in your Applications folder.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCreate an Xbar plugin\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn Xbar plugin is just a shell script that's placed in a special directory. When Xbar launches it will run these scripts.\u003c/p\u003e\n\u003cp\u003eGo to the xbar plugins directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Go to the xbar plugins directory\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e ~/Library/Application\\ Support/xbar/plugins\n\n\u003cspan class=\"hljs-comment\"\u003e# Create a bash file called runLlava.1d.sh\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003etouch\u003c/span\u003e runLlava.1d.sh\n\n\u003cspan class=\"hljs-comment\"\u003e# Make the file executable\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003echmod\u003c/span\u003e +x runLlava.1d.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we can add some code to \u003ccode class=\"inline-code-custom-style\"\u003erunLlava.1d.sh\u003c/code\u003e that calls the Llamafile executable you downloaded in step 1:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-meta\"\u003e#!/bin/bash\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003enohup\u003c/span\u003e /Users/paultreanor/ai/llamaFile/llamafile-server-0.1-llava-v1.5-7b-q4 \u0026#x26;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point you should test the \u003ccode class=\"inline-code-custom-style\"\u003erunLlava.1d.sh\u003c/code\u003e out.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\n./runLlava.1d.sh\n\n\u003cspan class=\"hljs-comment\"\u003e# Go to http://127.0.0.1:8080/ to make sure it's working\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eBringing it all together\u003c/strong\u003e\n\u003ca href=\"https://www.idownloadblog.com/2015/03/24/apps-launch-system-startup-mac/\"\u003eSet xbar to run on startup\u003c/a\u003e and then restart your machine.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThen bookmark localhost:8080 in your browser so you can access it quickly. The model will always be available to answer your questions, just like to ChatGPT...but without the downtime.\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\n\u003c/p\u003e\n\u003cimg src=\"/images/llamafile/bookmark.png\" alt=\"llamafile web UI and bookmark\"\u003e\n\u003ch2\u003eUpdate\u003c/h2\u003e\n\u003cp\u003eThis absolutely chews through my battery life even though it's idle in the background. I think making the script a little more event driven (maybe triggered by visiting port 8080) and not just running idle should be a decent fix for this.\u003c/p\u003e","title":"Automatically running LLMs at startup on Mac","short":"Setting up a local LLM on Mac so that it's always ready for your questions","date":"2023-12-01","slug":"llamafile","createdAt":"2023-12-01","img":"blog-2.jpg","tags":["Tutorial"]}},"__N_SSG":true},"page":"/[id]","query":{"id":"llamafile"},"buildId":"HU6vXSPKOkwvEroM8TGj-","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>